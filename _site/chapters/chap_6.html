<!DOCTYPE html>
<html lang="en-US">

  <head>
    <meta charset='utf-8'>
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width,maximum-scale=2">
    <link rel="stylesheet" type="text/css" media="screen" href="/assets/css/style.css?v=3463278a922aabe1a6006e8ea123dc42fad31a7c">

<!-- Begin Jekyll SEO tag v2.5.0 -->
<title>NLP in a Nut(s)shell | Readings &amp; materials for learning NLP from the ground up!</title>
<meta name="generator" content="Jekyll v3.8.5" />
<meta property="og:title" content="NLP in a Nut(s)shell" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Readings &amp; materials for learning NLP from the ground up!" />
<meta property="og:description" content="Readings &amp; materials for learning NLP from the ground up!" />
<link rel="canonical" href="http://localhost:4000/chapters/chap_6.html" />
<meta property="og:url" content="http://localhost:4000/chapters/chap_6.html" />
<meta property="og:site_name" content="NLP in a Nut(s)shell" />
<script type="application/ld+json">
{"description":"Readings &amp; materials for learning NLP from the ground up!","@type":"WebPage","url":"http://localhost:4000/chapters/chap_6.html","headline":"NLP in a Nut(s)shell","@context":"http://schema.org"}</script>
<!-- End Jekyll SEO tag -->

  </head>

  <body>

    <!-- HEADER -->
    <div id="header_wrap" class="outer">
        <header class="inner">
          
            <a id="forkme_banner" href="https://github.com/ricardorei/ricardorei.github.io">View on GitHub</a>
          

          <h1 id="project_title">NLP in a Nut(s)shell</h1>
          <h2 id="project_tagline">Readings & materials for learning NLP from the ground up!</h2>

          
        </header>
    </div>

    <!-- MAIN CONTENT -->
    <div id="main_content_wrap" class="outer">
      <section id="main_content" class="inner">
        <ul>
  <li><a href="/">Home</a></li>
</ul>

<h1 id="morphology-what-is-it-is-there-a-cure">Morphology? What is it? Is there a cure?</h1>

<blockquote>
  <p>Text by: <a href="authors.md">Luísa Coheur</a></p>
</blockquote>

<p><strong>Morphology</strong> is the linguistics field dedicated to the study of the internal
structure of words (morph = shape, logos = word).</p>

<p>     Being able to determine the morphology of words can be extremely important
to get, for instance, their accurate lemmas; it is also a fundamental
step should a syntactic analysis be required. Many applications are based on
some morphological processing. For instance, if you want to know whether
two words in different languages have the same origin (organization vs. organização), 
that is, if they are <strong>cognates</strong> (another fancy word to impress
your friends), morphology can help.</p>

<p align="center">
<img align="center" width="232" height="256" src="../images/morphology_meme.jpg" /> 
</p>

<p>     In this chapter we will study words’ constituents (morphologic parsing)
and we will see different techniques to mark words in a text with morphosyntactic tags.</p>

<p>     This chapter is particularly inspired by <a href="https://web.stanford.edu/~jurafsky/slp3/">Jurasfsky’s book</a>, 
previous <a href="https://www.l2f.inesc-id.pt/w/Nuno_Mamede">Nuno Mamede</a>’s slides and also the slides of Sudeshna Sarkar. All the silly jokes
and mistakes are on me.</p>

<h2 id="dissecting-words-into-morphemes">Dissecting words into morphemes</h2>

<p>Words are constituted by (meaningful) units called <strong>morphemes</strong>. There are
two types of morphemes:</p>
<ol>
  <li><strong>stems</strong>, which carry the primary of meaning of words;</li>
  <li><strong>affixes</strong>, which change stems meaning and/or have grammatical functions. They come in different types:
    <ul>
      <li><strong>prefixes:</strong> added at the beginning of the word. For instance, anti from <em>anti-motim</em>, or <em>des</em> from <em>desinteressante</em>;</li>
      <li><strong>suffixes:</strong> appear at the end of the word. An example is <em>os</em> from <em>gatos</em>, <em>alunos</em>, etc.</li>
      <li><strong>infixes:</strong> are inserted inside the stem</li>
      <li><strong>circumfixes:</strong> precede and follow the stem and are inserted at the same time. For instance, <em>amanhecer</em> has this type of affix.</li>
      <li><strong>clitics:</strong> these are morpheme that function like a word, but that do not appear alone. Example: <em>os</em> in <em>vi–os</em>.</li>
    </ul>
  </li>
</ol>

<p>     There are languages (called <strong>agglutinative Languages</strong>) where words can contain an impressive number of morphemes. 
Turkish, for instance, has many words with 9 or 10 morphemes.</p>

<p align="center">
<img align="center" src="../images/agglutinative.png" />
<figcaption> Image taken from <a href="http://specgram.com/CLII.3/09.phlogiston.cartoon.3.html">http://specgram.com/CLII.3/09.phlogiston.cartoon.3.html</a> </figcaption>
</p>

<hr />

<p><strong>Exercise 22: Dissecting Portuguese</strong></p>

<p>     Consider the word <em>inacreditavelmente</em>. Can you find a portuguese word with
more morphemes? If you do, let me know.</p>

<hr />

<h2 id="building-words">Building words</h2>

<p>There are many different ways of building words from a word stem. Here
are some of them:</p>

<ul>
  <li>Inflection: Doesn’t change the word class or the meaning of the word, considering the original stem. Examples are <em>eats</em> from <em>eat</em> (both verbs) and <em>gatas</em> from <em>gato</em> (both nouns).</li>
  <li>Derivation: Results in a word from a different word class or with a different meaning. Examples are <em>do</em> from <em>undo</em> (opposite meanings) and <em>amigável</em> from <em>amigo</em> (adjective and noun).</li>
  <li>Compounding: Combination of multiple word stems. <em>doghouse</em> and <em>guarda-chuva</em> are examples of this.</li>
  <li>Cliticization: Words with clitics, such as <em>apagou-o</em>.</li>
  <li>…</li>
</ul>

<p>     Just to show you how strange this can be, have the following example (for
Hebrew), representing a case of <strong>Templatic</strong> morphology with triconsonantal
stems:</p>

<p>     Considering that <em>lmd</em> means to learn or study:</p>
<ul>
  <li>CaCaC triggers lamad (<em>he studied, …</em>)</li>
  <li>CiCeC triggers limed (<em>he taught, …</em>)</li>
  <li>CuCaC triggers lumad (<em>he was taught, …</em>)</li>
</ul>

<p>     Original, isn’t it?</p>

<h2 id="part-of-speech-tagging">Part-of-speech tagging</h2>

<p><strong>Part-of-speech tagging</strong> or <strong>POS tagging</strong> is the process of automatically
assigning a morpho-syntactic tag to each word in a text. The main problem
here is our darling friend ambiguity, but not because there are many ambiguous
words. The problem is that these ambiguous words are extremely
frequent in texts.</p>

<p>     We will study three main approaches: a) Rule-based; b) Hybrid; c)
Stochastic.</p>

<h3 id="rule-based-approach">Rule-based approach</h3>

<p>The rule-based approach uses hand-crafted rules to tag a text. Usually,
there are two main steps:</p>
<ol>
  <li>With the help of a dictionary, you tag each word with all its possible labels;</li>
  <li>With the help of a set of (disambiguation) rules, you disambiguate these labels.</li>
</ol>

<p>     As an example, consider the sentence <em>He had a book</em>. After the first step,
you might have:</p>

<ul>
  <li>he he/pronoun</li>
  <li>had have/verbpast have/auxliarypast</li>
  <li>a a/article</li>
  <li>book book/noun book/verb</li>
</ul>

<p>     If there is a rule that states that <em>if the previous word is an article, remove
all the labels related with verbs</em>, after the application of this rule, we obtain:</p>

<ul>
  <li>he he/pronoun</li>
  <li>had have/verbpast have/auxliarypast</li>
  <li>a a/article</li>
  <li>book book/noun</li>
</ul>

<p>An example of a rule-based tagger is the <a href="http://www2.lingsoft.fi/doc/engcg/intro/">EngCG tagger</a>. If you have
time, give it a once-over.</p>

<h3 id="transformation-based-pos-tagging">Transformation based POS tagging</h3>

<p>The <strong>transformation based POS tagging</strong> is also known as <strong>Brill Tagging</strong>.
It is also rule-based, but its rules are learned from a labeled corpus
(that is why is said to be an hybrid approach). It takes the following steps:</p>

<ul>
  <li>Each word is tagged with the most frequent label;</li>
  <li>Transformational rules are learned from a labeled corpus (rules that replace labels);</li>
  <li>Transformational rules are applied until some stop condition is reached.</li>
</ul>

<p>For instance, assume that you know that:</p>

<script type="math/tex; mode=display">P(\text{NN/race}) = 0.98</script>

<script type="math/tex; mode=display">P(\text{VB/race}) = 0.02</script>

<p>     Then, the sentence <em>He is expected to race tomorrow</em> will be labeled as:</p>
<ul>
  <li>he/PRN</li>
  <li>is/VBZ</li>
  <li>expected/VBN</li>
  <li>to/TO</li>
  <li>race/NN (not good!)</li>
  <li>tomorrow/NN</li>
</ul>

<p>     If you have a rule that says that you should <em>Replace NN by VB when the
previous label is TO</em>, the correct labelling will be attained.</p>

<h3 id="stochastic-approach">Stochastic approach</h3>

<p>The target here is to choose the best sequence of tags,</p>

<script type="math/tex; mode=display">T = t_1, t_2, ..., t_n</script>

<p>     for a certain sequence of words</p>

<script type="math/tex; mode=display">W = w_1, w_2, ..., w_n</script>

<p>     That is, we want to calculate <script type="math/tex">t \in \tau</script> that maximizes <script type="math/tex">P(T | W)</script>, being <script type="math/tex">\tau</script>
the set of all possible tags’ sequences:</p>

<script type="math/tex; mode=display">\arg\max_{t \in \tau} P(T|W)</script>

<p>      As it is difficult to calculate 
<script type="math/tex">P(T | W)</script>
, we can use Bayes Rule:</p>

<script type="math/tex; mode=display">P(x|y) = P(y|x) \frac{P(x)}{P(y)}</script>

<p>     and as <script type="math/tex">P(W)</script> does not depend on <script type="math/tex">T</script>, our problem is now to calculate:</p>

<script type="math/tex; mode=display">\arg\max_{t \in \tau} P(W|T) \times \frac{P(T)}{P(W)} \equiv \arg\max_{t \in \tau} P(W|T) \times P(T)</script>

<p>Note that:</p>
<ul>
  <li><script type="math/tex">P(T)</script>
 is the prior probability of the sequence of tags;</li>
  <li><script type="math/tex">P(W | T)</script> 
is the likelihood of W being given T.</li>
</ul>

<hr />

<p><strong>Exercise 23: Why is posterior distribution hard to calculate?</strong></p>

<p>     Think why it is more difficult to calculate 
<script type="math/tex">P(T | W)</script>
 than 
 <script type="math/tex">P(W | T)</script>
.</p>

<hr />

<h4 id="the-hidden-markov-models">The Hidden Markov Models</h4>

<p>Unfortunately, the previous formula is still too difficult to calculate and
the famous <strong>Hidden Markov Models (HMM)</strong> are called to save the day.
These models assume that the system in hands can be modeled as a Markov
Process (remember?) with non observable (hidden) states. Basically, they
assume that:</p>

<ul>
  <li>The probability of occurrence of a word only depends on its label (that is, it doesn’t depend on other words);</li>
  <li>The probability of a tag only depends on the previous tag (this is called the bigram assumption).</li>
</ul>

<p>Due to this, the following approximations can be done:</p>

<script type="math/tex; mode=display">P(W|T) \approx \prod_{i=1}^n P(w_i|t_i)</script>

<script type="math/tex; mode=display">P(T) \approx \prod_{i=1}^n P(t_i|t_{i-1})</script>

<p>Therefore, the formula that we need to calculate is now given by:</p>

<script type="math/tex; mode=display">\arg\max_{t \in \tau} P(W|T) \times P(T) \approx \prod_{i=1}^n P(w_i|t_i)P(t_i|t_{i-1})</script>

<p>The good news is that we now know how to calculate this:</p>

<script type="math/tex; mode=display">P(t_i|t_{i-1}) = \frac{C(t_{i-1} t_i)}{C(t_{i-1})}</script>

<script type="math/tex; mode=display">P(w_i|t_i) = \frac{C(t_i, w_i)}{C(t_i)}</script>

<h4 id="viterbi-algorithm">Viterbi algorithm</h4>

<p>For models such as HMM that contain hidden variables, the task of determining
which of the variables is the source of an observable sequence is
called decoding. The Viterbi algorithm is often used with HMMs and
it seeks the best path; it is based on probabilities and uses the technique
of <strong>dynamic programming</strong> (solve small problems and memorize the best
solution).</p>

<p>     Considering that:</p>

<ul>
  <li><script type="math/tex">N = \#  \{L_1, ..., L_n\}</script>;</li>
  <li><script type="math/tex">n =</script> number of words in the given sequence <script type="math/tex">w_1, ..., w_n</script>;</li>
  <li>SS is an array <script type="math/tex">N \times n</script> that registers the best punctuation of the best sequence found until a certain position with label <script type="math/tex">L_i</script>;</li>
  <li>BP is an array <script type="math/tex">N \times n</script> that registers the best probability of a transition from the previous to the current state;</li>
  <li>C is an array <script type="math/tex">1 \times n</script> that registers the best sequence of labels.</li>
</ul>

<p>     Then, the Viterbi algorithm works as follows:</p>

<p align="center">
<img align="center" src="../images/vitterbi.png" width="625" height="600" /> 
</p>

<hr />

<p><strong>Exercise 24: Viterbi</strong></p>

<p>     Consider the labels N and V, and the sequence of words A B A. Knowing the
following probabilities, use Viterbi to calculate the best sequence of labels for that sequence of words:</p>

<ul>
  <li><script type="math/tex">% <![CDATA[
P(N | <s>) = 0.25 %]]></script>
, 
<script type="math/tex">% <![CDATA[
P(V | <s>) = 0.3 %]]></script></li>
  <li><script type="math/tex">P(A|N) = 0.1</script>
,
<script type="math/tex">P(A|V) = 0.3</script>
,
<script type="math/tex">P(B|N) = 0.2</script>
,
<script type="math/tex">P(B|V) = 0.4</script></li>
  <li><script type="math/tex">P(N|V) = 0.15</script>
,
<script type="math/tex">P(N|N) = 0.2</script>
,
<script type="math/tex">P(V|V) = 0.75</script>
,
<script type="math/tex">P(V|N) = 0.25</script></li>
</ul>

<hr />

<h2 id="very-brief-review-of-portuguese-morphology--back-to-school">Very brief review of Portuguese morphology – back to school</h2>

<p>Each word can have more than one morphological tag, according with its
function in a sentence. For instance, you can say <em>Estava uma noite escura |
branca | clara | amarela | verde</em>, but you can’t say <em>Estava uma noite de</em> or
<em>Estava uma noite dormir</em>. That is, if you have an adjective, you can replace
it by another adjective, but not with a preposition or a verb (an exception are
nouns and pronouns). You don’t remember what a pronoun or a preposition
is? Well, let us review some morphological classes (remember this, because
you will need it for the Syntax chapter); capitalized words illustrate each
class:</p>

<ul>
  <li>Nouns: <em>JOAQUIM estava a ler na SALA quando ouviu um BARULHO no JARDIM.</em></li>
  <li>Determiners: <em>Era UM ruído estranho, O mais estranho de sempre, UM leve e suave arrastar.</em></li>
  <li>Pronouns: <em>_Sem que ELE o conseguisse evitar, sentiu o SEU coração a começar a bater com força.</em></li>
  <li>Verbs: <em>DIRIGIU-SE à porta, mas PAROU, com a maçaneta na mão.</em></li>
  <li>Adjectives: <em>Sentia um pavor INDESCRITÍVEL a entranhar-se no corpo.</em></li>
  <li>Adverbs: <em>LENTA e FIRMEMENTE um frio glaciar subia por ele acima.</em></li>
  <li>Prepositions: <em>O que estaria DO outro lado DA porta? O que estaria NO jardim?</em></li>
  <li>Conjunction: <em>Tremendo, Joaquim abriu a porta E espreitou.</em></li>
  <li>Interjections: <em>Nem teve tempo de gritar “AQUI D’EL REI”, “DIACHO” ou “IRRA”.</em></li>
</ul>

<div>
  <a href="chap_5.html" style="float: left;">❮ Previous chapter</a>
  <a href="chap_7.html" style="float: right;">Next chapter ❯</a>
</div>

<p><br /><br /></p>

      </section>
    </div>

    <!-- FOOTER  -->
    <div id="footer_wrap" class="outer">
      <footer class="inner">
        
        <p class="copyright">NLP in a Nut(s)shell maintained by <a href="https://github.com/ricardorei">ricardorei</a></p>
        
        <p>Published with <a href="https://pages.github.com">GitHub Pages</a></p>
      </footer>
    </div>

    <script type="text/javascript" src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>

    
  </body>
</html>